---
title: O que é Inteligência Artificial?
description: Uma introdução aos conceitos fundamentais da Inteligência Artificial e sua evolução histórica
---

# Introdução

O conceito de IA não é algo recente, mas nos últimos anos, houve um _boom_ no uso desse termo. Nesse curso você vai entender o porquê desse _boom_ e como isso está mudando o mundo.

O objetivo desse curso é familiarizar você com os conceitos fundamentais da IA e sua aplicação prática no cotidiano. Para que você possa entender como isso impacta sua vida e para que você possa se beneficiar desse conhecimento, entendendo as implicações éticas e sociais dessa tecnologia.

Esse curso não vai te tornar um especialista em IA, até por que esse é um campo de pesquisa muito extenso que envolve diversas áreas como a Ciência da Computação, Matemática, Estatística, Filosofia, Psicologia e outras.

Mas a minha intenção é que ao final desse curso, você entenda o que é a IA e como você pode utilizar dos seus benefícios, mesmo sem ser um expert.

## O que é a IA?

Basicamente, a Inteligência Artificial (IA) é um campo da ciência da computação que busca criar sistemas capazes de realizar tarefas que normalmente requerem inteligência humana.

Desde 1950, pesquisadores como Alan Turing começaram a estudar como poderiam criar máquinas que fossem capazes de realizar esse tipo de tarefa. Foi daí que surgiu o famoso [Teste de Turing](https://pt.wikipedia.org/wiki/Teste_de_Turing), onde coloca-se uma máquina e um humano para responder perguntas de um terceiro. Se esse terceiro não consegue distinguir de quem é a resposta, diz-se que a máquina passou no teste.

Ao longo das décadas, a pesquisa em IA foi se desenvolvendo e se subdividindo em áreas diferentes, como:
- Aprendizado de máquina
- Processamento de linguagem natural
- Visão computacional
- Robótica
- entre outras

Todas essas sub-categorias somadas, tendem a chegar no objetivo final da pesquisa em IA: criar a [inteligência artifical geral (AGI)](https://pt.wikipedia.org/wiki/Inteligência_artificial_geral), que seria a capacidade de uma máquina completar qualquer tarefa que um humano pode fazer.

Ao mesmo tempo, essas sub-categorias podem ser aplicadas para resolução de problemas específicos, como:
- Visão computacional: Carros autônomos
- Processamento de linguagem natural: Reconhecimento de voz, tradução de idiomas em tempo real, etc
- Aprendizado de máquina: Recomendação de produtos, classificação de e-mails como spam ou não, etc

Mas se essa pesquisa em IA existe há tanto tempo, por que só agora parece que está todo mundo falando sobre ela?

## A linha do tempo da pesquisa em IA

Para entender isso, vamos traçar uma linha do tempo bem resumida da pesquisa em IA.

### Década de 1950
Como dito anteriormente, os primórdios da pesquisa em IA remontam à década de 50, com pesquisadores como Alan Turing, McCulloch e Pitts.

Em 1956 o termo "Inteligência Artificial" foi criado pelo cientista da computação John McCarthy, que é considerado um dos "pais da IA".

### Década de 1960 até 1990

Ao longo das próximas décadas, a pesquisa em IA foi se desenvolvendo com o surgimento de novas linguagens de programação, como Lisp, e com o surgimento de novos algoritmos e técnicas. Porém as aplicações práticas ainda eram limitadas, até por conta da tecnologia disponível na época: por exemplo, ainda não existiam celulares.

### Década de 1990 até 2020

A partir da década de 90, houve um redirecionamento da pesquisa para técnicas de aprendizado de máquina, que são técnicas que permitem que as máquinas aprendam com dados. Esses algoritmos se mostraram mais promissores.

Em paralelo, houve o _boom_ da internet: a tecnologia evoluiu e se popularizou e as pessoas passaram a ter acesso a computadores pessoais e depois aos celulares e smartphones. Além disso, houve o surgimento das redes sociais.

Esse fenômeno da popularização da internet foi muito importante para o desenvolvimento da IA, pois, como dito anteriormente, algoritmos de aprendizado de máquina precisam de dados para aprender. E com a popularização da internet, foi possível reunir um grande volume de dados.

Tudo o que você faz online, tudo o que você posta no Facebook, tudo o que você compra no Amazon, tudo o que você procura no Google, tudo isso são dados que podem alimentar o aprendizado de uma máquina. E é daí que surgem os primeiros dilemas éticos relacionados à IA, que nós vamos explorar mais a frente no curso. Afinal quem são os donos desses dados?

Outro evento que foi importante para essa história, foi a evolução do hardware disponível. Para quem não sabe a diferença entre software e hardware: o software é o que você usa para interagir com uma máquina, seja um programa de computador, um app de celular, um site, etc. O hardware são os componentes físicos da máquina, como o teclado, o mouse, a placa mãe, a memória, etc. Tem uma piada antiga que diz que software é tudo o que você xinga e o hardware é tudo que você chuta. Mas brincadeiras à parte, o componente de hardware mais importante no desenvolvimento de IA é a GPU (mais conhecida como placa de vídeo).

Como o próprio nome já diz, a placa de vídeo foi criada com a intenção de processar imagens e vídeos, mas a forma como ela processa esses dados é muito eficiente para os algoritmos de aprendizado de máquina. Ou seja, com a evolução das GPUs, a IA evoluiu em conjunto.

> Uma curiosidade: a NVIDIA que é a principal fabricante de GPUs, está hoje entre as 10 empresas mais valiosas do mundo, tendo ocupado a primeira posição em 2024.

O próximo grande marco na linha do tempo da IA foi o surgimento dos algoritmos de aprendizado profundo (deep learning), que são algoritmos que permitem que as máquinas aprendam com dados de forma muito mais eficiente.

Não faz sentido para esse curso aprofundar muito em como funcionam esses algoritmos, mas vale entender que eles demonstraram um resultado muito superior aos algoritmos anteriores. E isso, somado aos grandes volumes de dados disponíveis, fez com que a pesquisa em IA ganhasse um novo patamar.

### Década de 2020 e atualidade

E foi aí que em 2020 aconteceu o grande salto da IA: o surgimento dos LLMs (Large Language Models ou Modelos de Linguagem Grandes).

Talvez você conheça esses modelos como "ChatGPT", que foi o produto responsável por tornar acessível o uso dessa tecnologia.

Então na prática, o famoso ChatGPT é uma interface que permite que você interaja com o que há de mais moderno no mundo da IA: os LLMs. Nós vamos entrar em mais detalhes sobre o que são esses LLMs, mas para fechar a nossa linha do tempo, vamos antes nos situar com o que aconteceu nos últimos anos.

Junto aos LLMs, outras tecnologias começaram a ganhar destaque, como:
- DALL-E, Stable Diffusion e Midjourney: modelos que geram imagens a partir de um texto
- Whisper: um modelo que transcreve áudio para texto
- Sora: modelo que gera vídeos a partir de um texto
- entre outros

Todas essas tecnologias têm em comum o fato de que usam algoritmos de aprendizado de máquina que são impulsionados por um grande volume de dados e um grande poder de computação, como as GPUs.

E agora estamos vivendo um momento onde existe uma corrida entre as grandes empresas, como Google, OpenAI, Meta, etc, para ver quem vai dominar esse mercado criando os melhores modelos. E do outro lado, existe uma corrida entre empresas para criar aplicações que estejam integradas com esses modelos.


## Redes neurais



LLMs -> 2 arquivos: parameters + 500 lines of C code

redes neurais

model training -> 10tb of text -> gpu cluster 6000 gpus for 12 days -> 2m dollars -> parameters 140gb (llama 70b)

uma rede neural basicamente tentar prever qual é a próxima palavra em uma sequência de palavras

o resultado de uma rede neural é uma junção de palavras que estatisticamente estão relacionadas com o prompt fornecido. é como se a rede criasse um texto com palavras que fazem sentido, estatisticamente.

mas isso quer dizer que o resultado não é necessariamente correto. por exemplo, se você pedir para a rede gerar o resultado da loteria, ela vai te dar alguns números, mas obviamente não vai ser correto necessariamente. ela vai entender que vc quer numeros de loteria, mas não consegue prever o resultado.

esse é um exemplo exagerado, mas que pode se aplicar em outros contextos para você entender que nem sempre pode confiar nos resultados de uma IA. pense em outros casos: 

- você pode falar para a rede uma série de sintomas e perguntar qual é o seu diagnóstico. ela pode ter informações de várias doenças e vai te dar um diagnóstico baseado nessas informações. mas pode ser que ela te dê uma resposta incorreta. e com isso você pode tomar uma decisão errada. nós chamaos esses erros de alucinações.

a forma como o treinamento das redes neurais acontece não é 100% compreendido. os algoritmos vão otimizando os resultado, mas n é possível determinar como os parâmetros estão se relacionando dentro da rede.